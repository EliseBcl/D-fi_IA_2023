# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MxOMFPTy6hA8kvFXzFY-gMblh2PKw1jc

# Train.py
"""

# !git clone https://github.com/EliseBcl/Defi_IA_2023.git

"""## Data Initialization"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from math import sqrt, log
import seaborn as sns
from sklearn.preprocessing import scale
from pandas.plotting import scatter_matrix
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler  
from collections import Counter
from sklearn.datasets import make_classification
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import load_iris
#from factor_analyzer import FactorAnalyzer
from sklearn.feature_extraction import DictVectorizer
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
from xgboost import cv
import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 12})
from sklearn.ensemble import GradientBoostingRegressor

Matrix1 = pd.read_csv('/requests/requests_1.csv',index_col=0)
Matrix2 = pd.read_csv('/requests/requests_2.csv',index_col=0)
Matrix3 = pd.read_csv('/requests/requests_3.csv',index_col=0)
Matrix4 = pd.read_csv('/requests/requests_4.csv',index_col=0)
Matrix5 = pd.read_csv('/requests/requests_5.csv',index_col=0)

Matrix1 = Matrix1.assign(order_request=1)
Matrix2 = Matrix2.assign(order_request=2)
Matrix4 = Matrix4.assign(order_request=4)
Matrix5 = Matrix5.assign(order_request=5)

def assign_requests_order(copy_requests):
  requests = copy_requests.copy()

  #def DF requests order
  requests_order = pd.DataFrame(requests.index)
  requests_order.columns = ['order_request']
  requests_order.request_order = 0

  #modif order 
  for n in requests.name.unique():
    for d in requests[requests.name == n].date.unique():
      requests_order[(requests.name == n).values * (requests.date == d).values] = np.where(requests[requests.name == n].date.unique() == d)[0] + 1 
  return requests_order

for name_r in ['requests_6_part1', 'requests_6_part2', 'requests_7_part1', 'requests_7_part2', 'requests_7_part3',
               'requests_8_part1', 'requests_8_part2', 'requests_8_part3', 'requests_9_part1', 'requests_9_part2', 'requests_9_part3']:
  r = pd.read_csv('/requests/'+name_r+'.csv',index_col=0)
  b = r.copy()
  c = assign_requests_order(b)
  r = pd.read_csv('/requests/'+name_r+'.csv',index_col=0)
  r = r.join(c)
  r.to_csv('/requests/'+ name_r + '_order.csv')

#request 6
Matrix6_part1 = pd.read_csv('/requests/'+'requests_6_part1'+'_order.csv',index_col=0)
# display(Matrix6_part1)
Matrix6_part2 = pd.read_csv('/requests/'+'requests_6_part2'+'_order.csv',index_col=0)
# display(Matrix6_part2)
#requests 7
Matrix7_part1 = pd.read_csv('/requests/requests_7_part1_order.csv',index_col=0)
Matrix7_part2 = pd.read_csv('/requests/requests_7_part2_order.csv',index_col=0)
Matrix7_part3 = pd.read_csv('/requests/requests_7_part3_order.csv',index_col=0)
#requests 8
Matrix8_part1 = pd.read_csv('/requests/requests_8_part1_order.csv',index_col=0)
Matrix8_part2 = pd.read_csv('/requests/requests_8_part2_order.csv',index_col=0)
Matrix8_part3 = pd.read_csv('/requests/requests_8_part3_order.csv',index_col=0)
#requests 9
Matrix9_part1 = pd.read_csv('/requests/requests_9_part1_order.csv',index_col=0)
Matrix9_part2 = pd.read_csv('/requests/requests_9_part2_order.csv',index_col=0)
Matrix9_part3 = pd.read_csv('/requests/requests_9_part3_order.csv',index_col=0)

data = pd.concat([Matrix1, Matrix2, Matrix4, Matrix5, Matrix6_part1, Matrix6_part2, Matrix8_part1, Matrix8_part2, Matrix8_part3])

data.duplicated().sum()

data.drop_duplicates(keep = 'first', inplace=True)

data.duplicated().sum()

"""## Functions initialization"""

def to_dummies(data_test):
  # merge on hotel id
  hotels = pd.read_csv('/data/features_hotels.csv',index_col=0)
  hotels = hotels.drop(['city'], axis = 1)
  data_test = data_test.join(hotels, on = 'hotel_id')

  # to categorical
  data_test['city'] = pd.Categorical(data_test.city)
  data_test['language'] = pd.Categorical(data_test.language)
  data_test['mobile'] = pd.Categorical(data_test.mobile)
  data_test['avatar_id'] = pd.Categorical(data_test.avatar_id)
  data_test['group'] = pd.Categorical(data_test.group)
  data_test['brand'] = pd.Categorical(data_test.brand)
  data_test['parking'] = pd.Categorical(data_test.parking)
  data_test['pool'] = pd.Categorical(data_test.pool)
  data_test['children_policy'] = pd.Categorical(data_test.children_policy)

  # dummies
  liste= ['stock', 'date','order_request', 'mobile', 'parking', 'pool']

  X_testDum = pd.get_dummies(data_test[["city","language","group","brand","children_policy"]])
  X_testQuant = data_test[liste]
  X_test_dum = pd.concat([X_testDum,X_testQuant],axis=1)

  return X_test_dum

def transfo_stock(X):
  mean_stock = np.mean(np.array(np.log(X+0.001)))
  std_stock = np.std(np.array(np.log(X+0.001)))
  return (np.array(np.sqrt(X+0.001))- mean_stock)/std_stock

def transfo_price(X):
  return np.log(X+0.001)

col = ['city_amsterdam', 'city_copenhagen', 'city_madrid', 'city_paris',
       'city_rome', 'city_sofia', 'city_valletta', 'city_vienna',
       'city_vilnius', 'language_austrian', 'language_belgian',
       'language_bulgarian', 'language_croatian', 'language_cypriot',
       'language_czech', 'language_danish', 'language_dutch',
       'language_estonian', 'language_finnish', 'language_french',
       'language_german', 'language_greek', 'language_hungarian',
       'language_irish', 'language_italian', 'language_latvian',
       'language_lithuanian', 'language_luxembourgish', 'language_maltese',
       'language_polish', 'language_portuguese', 'language_romanian',
       'language_slovakian', 'language_slovene', 'language_spanish',
       'language_swedish', 'group_Accar Hotels', 'group_Boss Western',
       'group_Chillton Worldwide', 'group_Independant',
       'group_Morriott International', 'group_Yin Yang', 'brand_8 Premium',
       'brand_Ardisson', 'brand_Boss Western', 'brand_Chill Garden Inn',
       'brand_Corlton', 'brand_CourtYord', 'brand_Ibas', 'brand_Independant',
       'brand_J.Halliday Inn', 'brand_Marcure', 'brand_Morriot',
       'brand_Navatel', 'brand_Quadrupletree', 'brand_Royal Lotus',
       'brand_Safitel', 'brand_Tripletree', 'children_policy_0',
       'children_policy_1', 'children_policy_2', 'stock', 'date',
       'order_request', 'mobile', 'parking', 'pool']

"""### Train set"""

def format_train(X):
  X_train = X.copy()
  Y_train = X_train['price']
  X_train = X_train.drop(['price'], axis = 1)
  X_train = to_dummies(X_train)
  X_train.stock = transfo_stock(X_train.stock)
  Y_train = transfo_price(Y_train)
  return X_train[col], Y_train

train, labels = format_train(data.copy())
display(train)
# display(labels)

"""### Test set"""

def format_test(X):
  X_test = X.copy()

  # order request
  X_test = X_test.assign(order_request=0)
  X_test['order_request'] = 1
  for avatar in np.unique(X_test['avatar_id']):
      X_test.loc[X_test['avatar_id'] == avatar, 'order_request'] = X_test['order_requests'].loc[X_test['avatar_id']== avatar] - min(X_test['order_requests'].loc[X_test['avatar_id']== avatar])+1
  X_test = X_test.drop(['order_requests'], axis = 1)

  X_test = to_dummies(X_test)
  X_test.stock = transfo_stock(X_test.stock)
  return X_test[col]

X_test = pd.read_csv('/data/test_set.csv',index_col=0)
test = format_test(X_test.copy())
display(test)

# train.to_csv('/content/Defi_IA_2023/trainset.csv')
# labels.to_csv('/content/Defi_IA_2023/labelstrain.csv')
# test.to_csv('/content/Defi_IA_2023/testset.csv')

Xtrain = pd.read_csv('/data/trainset.csv',index_col=0)
Ytrain = pd.read_csv('/data/labelstrain.csv',index_col=0)
Xtest = pd.read_csv('/data/testset.csv',index_col=0)

"""## Pipeline"""

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

gbm = make_pipeline(GradientBoostingRegressor(n_estimators=500))
gbmOpt=gbm.fit(Xtrain, Ytrain)

"""## Predictions"""

prediction = gbmOpt.predict(Xtest)

result = np.exp(prediction) - 0.001
submission = pd.DataFrame()
submission.index= X_test.index
submission['price'] = pd.DataFrame(result)
submission.index.name = 'index'

display(submission)

# submission.to_csv('/content/Defi_IA_2023/final_predictions4.csv')

